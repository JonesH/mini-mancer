"""
Integration Testing - End-to-end tests for Mini-Mancer system

Tests complete workflows from start to finish:
- Full bot creation and deployment pipeline
- OpenServ API integration
- Real Telegram bot interactions
- Error handling and recovery workflows
- Cross-component integration
"""

import asyncio
import pytest
import time
from typing import Dict, Any

from telegram import Bot
from telegram.error import TelegramError
from telegram.ext import Application

from conftest import BotTestSession, TelegramTestError


@pytest.mark.integration
@pytest.mark.network
@pytest.mark.asyncio
class TestEndToEndBotWorkflows:
    """Test complete bot creation and deployment workflows"""
    
    async def test_complete_bot_lifecycle(
        self,
        telegram_bot_session: BotTestSession,
        bot_interaction_helper,
        prototype_agent_instance,
        performance_monitor
    ):
        """Test complete bot lifecycle from creation to interaction"""
        print("üîÑ Testing complete bot lifecycle...")
        
        performance_monitor.start_timing("complete_lifecycle")
        
        try:
            # Step 1: Create bot via factory
            print("üìù Step 1: Creating bot via factory...")
            creation_result = await bot_interaction_helper.test_bot_creation_flow(
                "Create a helpful assistant bot named LifecycleBot",
                "LifecycleBot"
            )
            
            assert creation_result["success"], "Bot creation should succeed"
            print("‚úÖ Bot created successfully")
            
            # Step 2: Verify bot is tracked in agent
            print("üìä Step 2: Verifying bot state management...")
            agent = prototype_agent_instance
            
            if hasattr(agent, 'active_created_bot'):
                assert agent.active_created_bot is not None, "Bot should be tracked in agent"
                print("‚úÖ Bot properly tracked in agent")
            
            # Step 3: Test bot startup (simulated)
            print("üöÄ Step 3: Testing bot startup process...")
            await asyncio.sleep(5)  # Simulate startup time
            print("‚úÖ Bot startup completed")
            
            # Step 4: Test interaction with created bot (if available)
            if telegram_bot_session.created_bot_token:
                print("üí¨ Step 4: Testing interaction with created bot...")
                created_bot = Bot(token=telegram_bot_session.created_bot_token)
                
                try:
                    await created_bot.send_message(
                        chat_id=telegram_bot_session.test_chat_id,
                        text="Hello from LifecycleBot!"
                    )
                    print("‚úÖ Bot interaction successful")
                finally:
                    await created_bot.close()
            else:
                print("‚ö†Ô∏è Step 4: Skipping bot interaction (no created bot token)")
            
            # Step 5: Cleanup verification
            print("üßπ Step 5: Testing cleanup process...")
            if hasattr(agent, 'shutdown'):
                await agent.shutdown()
            print("‚úÖ Cleanup completed")
            
            duration = performance_monitor.end_timing("complete_lifecycle")
            
            print(f"üìä Complete Lifecycle Test Results:")
            print(f"   Duration: {duration:.2f}s")
            print(f"   All steps completed successfully")
            
            # Performance checks
            assert duration <= 60, f"Complete lifecycle took too long: {duration}s"
            
            print("‚úÖ Complete bot lifecycle test passed")
            
        except Exception as e:
            pytest.fail(f"Bot lifecycle test failed: {e}")
    
    async def test_factory_to_created_bot_handoff(
        self,
        telegram_bot_session: BotTestSession,
        bot_interaction_helper
    ):
        """Test handoff from factory bot to created bot"""
        print("ü§ù Testing factory to created bot handoff...")
        
        # Create bot via factory
        creation_result = await bot_interaction_helper.test_bot_creation_flow(
            "Create HandoffBot for testing",
            "HandoffBot"
        )
        
        assert creation_result["success"], "Bot creation should succeed for handoff test"
        
        # Verify factory bot responds normally after creation
        post_creation_response = await bot_interaction_helper.send_message_and_wait(
            "How are you doing after creating the bot?"
        )
        
        assert post_creation_response.message_id > 0, "Factory bot should respond after bot creation"
        
        print("‚úÖ Factory to created bot handoff test completed")
    
    async def test_multiple_bot_creation_sequence(
        self,
        telegram_bot_session: BotTestSession,
        bot_interaction_helper,
        performance_monitor
    ):
        """Test creating multiple bots in sequence"""
        print("üî¢ Testing multiple bot creation sequence...")
        
        performance_monitor.start_timing("multiple_bot_sequence")
        
        bot_names = ["SequenceBot1", "SequenceBot2", "SequenceBot3"]
        creation_results = []
        
        try:
            for i, bot_name in enumerate(bot_names):
                print(f"üìù Creating bot {i+1}: {bot_name}")
                
                result = await bot_interaction_helper.test_bot_creation_flow(
                    f"Create {bot_name} for sequence testing",
                    bot_name
                )
                
                creation_results.append(result)
                
                # Verify each creation
                assert result["success"], f"Bot creation {i+1} should succeed"
                
                # Wait between creations
                await asyncio.sleep(3)
            
            duration = performance_monitor.end_timing("multiple_bot_sequence")
            
            successful_creations = sum(1 for r in creation_results if r["success"])
            
            print(f"üìä Multiple Bot Creation Results:")
            print(f"   Duration: {duration:.2f}s")
            print(f"   Successful creations: {successful_creations}/{len(bot_names)}")
            print(f"   Average time per bot: {duration/len(bot_names):.2f}s")
            
            # All bots should be created successfully
            assert successful_creations == len(bot_names), "All bot creations should succeed"
            
            print("‚úÖ Multiple bot creation sequence test completed")
            
        except Exception as e:
            pytest.fail(f"Multiple bot creation sequence failed: {e}")


@pytest.mark.integration
@pytest.mark.api
@pytest.mark.asyncio
class TestAPIIntegration:
    """Test API integration points"""
    
    async def test_openserv_integration_simulation(
        self,
        prototype_agent_instance
    ):
        """Test OpenServ API integration (simulated)"""
        print("üîó Testing OpenServ API integration...")
        
        agent = prototype_agent_instance
        
        # Verify FastAPI app is available
        assert hasattr(agent, 'app'), "Agent should have FastAPI app"
        assert agent.app is not None, "FastAPI app should be initialized"
        
        # Test basic app configuration
        app = agent.app
        assert app.title == "Mini-Mancer Prototype", "App should have correct title"
        
        # In a full test, you would:
        # - Start the FastAPI server
        # - Make HTTP requests to test endpoints
        # - Verify responses and integration
        
        print("‚úÖ OpenServ API integration test completed (simulated)")
    
    async def test_agno_ai_integration(
        self,
        prototype_agent_instance
    ):
        """Test Agno-AGI integration"""
        print("üß† Testing Agno-AGI integration...")
        
        agent = prototype_agent_instance
        
        # Verify AI agent is available
        if hasattr(agent, 'agno_agent') and agent.agno_agent:
            print("‚úÖ Agno agent is available")
            
            # Test basic AI functionality
            try:
                # This would test actual AI responses in full implementation
                test_prompt = "Hello, please respond with a simple greeting"
                
                # In full implementation:
                # response = agent.agno_agent.run(test_prompt)
                # assert response.content, "AI should provide response"
                
                print("‚úÖ AI integration verified")
                
            except Exception as e:
                print(f"‚ö†Ô∏è AI integration test error: {e}")
        else:
            print("‚ö†Ô∏è Agno agent not available for testing")
        
        print("‚úÖ Agno-AGI integration test completed")


@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.asyncio
async def test_system_resilience_and_recovery(
    telegram_bot_session: BotTestSession,
    bot_interaction_helper,
    prototype_agent_instance
):
    """Test system resilience and error recovery"""
    print("üõ°Ô∏è Testing system resilience and recovery...")
    
    # Test recovery from various error conditions
    error_scenarios = [
        ("network_simulation", "Simulated network error"),
        ("invalid_input", "ü§ñ" * 100),  # Problematic input
        ("rapid_requests", "Multiple rapid requests"),
    ]
    
    recovery_results = []
    
    for scenario_name, test_input in error_scenarios:
        print(f"üí• Testing recovery from: {scenario_name}")
        
        recovery_start = time.time()
        
        try:
            if scenario_name == "rapid_requests":
                # Test rapid request handling
                for _ in range(5):
                    await bot_interaction_helper.send_message_and_wait("Quick test")
                    await asyncio.sleep(0.1)
            else:
                # Send problematic input
                await bot_interaction_helper.send_message_and_wait(test_input)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Expected error for {scenario_name}: {e}")
        
        # Test recovery with normal request
        try:
            recovery_test = await bot_interaction_helper.send_message_and_wait(
                "Recovery test - are you working normally?"
            )
            
            recovery_time = time.time() - recovery_start
            recovery_results.append((scenario_name, recovery_time, True))
            
            print(f"üîÑ Recovered from {scenario_name} in {recovery_time:.2f}s")
            
        except Exception as e:
            recovery_time = time.time() - recovery_start
            recovery_results.append((scenario_name, recovery_time, False))
            print(f"‚ùå Failed to recover from {scenario_name}: {e}")
        
        # Wait between scenarios
        await asyncio.sleep(2)
    
    # Analyze recovery results
    successful_recoveries = sum(1 for _, _, success in recovery_results if success)
    avg_recovery_time = sum(time for _, time, _ in recovery_results) / len(recovery_results)
    
    print(f"üìä System Resilience Test Results:")
    print(f"   Scenarios tested: {len(error_scenarios)}")
    print(f"   Successful recoveries: {successful_recoveries}/{len(error_scenarios)}")
    print(f"   Average recovery time: {avg_recovery_time:.2f}s")
    
    # Recovery requirements
    assert successful_recoveries >= len(error_scenarios) * 0.8, "Should recover from at least 80% of error scenarios"
    assert avg_recovery_time <= 10, f"Average recovery time too slow: {avg_recovery_time}s"
    
    print("‚úÖ System resilience and recovery test completed")


@pytest.mark.integration
@pytest.mark.network
@pytest.mark.asyncio
async def test_comprehensive_system_validation(
    telegram_bot_session: BotTestSession,
    bot_interaction_helper,
    prototype_agent_instance,
    performance_monitor
):
    """Comprehensive validation of entire system"""
    print("üîç Starting comprehensive system validation...")
    
    performance_monitor.start_timing("comprehensive_validation")
    
    validation_steps = {
        "factory_bot_health": False,
        "bot_creation": False,
        "agent_integration": False,
        "error_handling": False,
        "performance": False
    }
    
    try:
        # 1. Factory Bot Health Check
        print("üè• Step 1: Factory bot health check...")
        health_response = await bot_interaction_helper.send_message_and_wait("/start")
        validation_steps["factory_bot_health"] = health_response.message_id > 0
        print(f"   Factory bot health: {'‚úÖ PASS' if validation_steps['factory_bot_health'] else '‚ùå FAIL'}")
        
        # 2. Bot Creation Validation
        print("ü§ñ Step 2: Bot creation validation...")
        creation_result = await bot_interaction_helper.test_bot_creation_flow(
            "Create ValidationBot for comprehensive testing",
            "ValidationBot"
        )
        validation_steps["bot_creation"] = creation_result["success"]
        print(f"   Bot creation: {'‚úÖ PASS' if validation_steps['bot_creation'] else '‚ùå FAIL'}")
        
        # 3. Agent Integration Check
        print("üîß Step 3: Agent integration check...")
        agent = prototype_agent_instance
        has_agent = agent is not None
        has_app = hasattr(agent, 'app') and agent.app is not None
        validation_steps["agent_integration"] = has_agent and has_app
        print(f"   Agent integration: {'‚úÖ PASS' if validation_steps['agent_integration'] else '‚ùå FAIL'}")
        
        # 4. Error Handling Validation
        print("‚ö†Ô∏è Step 4: Error handling validation...")
        try:
            await bot_interaction_helper.send_message_and_wait("Invalid test input " + "x" * 100)
            # Should handle gracefully
            validation_steps["error_handling"] = True
        except Exception:
            # Graceful error handling is also acceptable
            validation_steps["error_handling"] = True
        print(f"   Error handling: {'‚úÖ PASS' if validation_steps['error_handling'] else '‚ùå FAIL'}")
        
        # 5. Performance Check
        print("‚ö° Step 5: Performance validation...")
        perf_start = time.time()
        for i in range(3):
            await bot_interaction_helper.send_message_and_wait(f"Performance test {i+1}")
            await asyncio.sleep(1)
        perf_duration = time.time() - perf_start
        validation_steps["performance"] = perf_duration <= 20  # Should complete in reasonable time
        print(f"   Performance ({perf_duration:.2f}s): {'‚úÖ PASS' if validation_steps['performance'] else '‚ùå FAIL'}")
        
        duration = performance_monitor.end_timing("comprehensive_validation")
        
        # Calculate overall system health
        passed_steps = sum(1 for step, passed in validation_steps.items() if passed)
        total_steps = len(validation_steps)
        system_health = (passed_steps / total_steps) * 100
        
        print(f"üìä Comprehensive System Validation Results:")
        print(f"   Duration: {duration:.2f}s")
        print(f"   Steps passed: {passed_steps}/{total_steps}")
        print(f"   System health: {system_health:.1f}%")
        
        for step, passed in validation_steps.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"   {step}: {status}")
        
        # System should pass most validation steps
        assert system_health >= 80, f"System health too low: {system_health}%"
        assert validation_steps["factory_bot_health"], "Factory bot must be healthy"
        assert validation_steps["agent_integration"], "Agent integration must work"
        
        print("‚úÖ Comprehensive system validation completed successfully")
        
    except Exception as e:
        pytest.fail(f"Comprehensive system validation failed: {e}")